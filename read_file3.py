import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin, urlparse
import time
import json
from datetime import datetime
import requests

class UltimateTCASScraper:
    def __init__(self):
        self.base_url = "https://www.mytcas.com"
        self.programs_data = []
        self.browser = None
        self.page = None
        self.session = requests.Session()
        
        # User agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
        self.target_keywords = [
            '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', 'computer', '‡∏Ñ‡∏≠‡∏°',
            '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå', 'artificial intelligence', 'ai',
            '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°', 'engineering'
        ]

    async def init_browser(self, headless=False, slow_mo=500):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô browser"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô browser...")
        
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=headless,
            slow_mo=slow_mo,
            args=['--start-maximized', '--no-sandbox', '--disable-blink-features=AutomationControlled']
        )
        
        self.context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        
        self.page = await self.context.new_page()
        self.page.set_default_timeout(30000)
        
        print("‚úÖ Browser ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")

    async def close_browser(self):
        """‡∏õ‡∏¥‡∏î browser"""
        try:
            if self.browser:
                await self.browser.close()
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: {e}")

    async def multi_method_data_collection(self):
        """‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        print("üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ...")
        
        all_links = []
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå
        print("\nüìç ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå")
        web_links = await self.search_via_website()
        all_links.extend(web_links)
        print(f"   ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {len(web_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡πá‡∏ö
        print("\nüìç ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡πá‡∏ö")
        structure_links = await self.analyze_website_structure()
        all_links.extend(structure_links)
        print(f"   ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {len(structure_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏•‡∏≠‡∏á URL patterns
        print("\nüìç ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏•‡∏≠‡∏á URL patterns")
        pattern_links = await self.try_url_patterns()
        all_links.extend(pattern_links)
        print(f"   ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {len(pattern_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 4: ‡πÉ‡∏ä‡πâ BeautifulSoup
        print("\nüìç ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 4: ‡πÉ‡∏ä‡πâ BeautifulSoup")
        bs_links = await self.beautifulsoup_method()
        all_links.extend(bs_links)
        print(f"   ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {len(bs_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")
        
        # ‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ã‡πâ‡∏≥
        unique_links = list(set(all_links))
        print(f"\nüìä ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(unique_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥)")
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå
        if unique_links:
            await self.extract_comprehensive_data(unique_links)
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
            await self.use_sample_data()

    async def search_via_website(self):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå"""
        links = []
        
        try:
            await self.page.goto(self.base_url, wait_until='networkidle')
            await self.page.wait_for_timeout(3000)
            
            search_terms = [
                '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå',
                'computer engineering', 
                '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå',
                'artificial intelligence engineering',
                'ai engineering',
                'com engineering'
            ]
            
            for term in search_terms:
                print(f"   üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {term}")
                term_links = await self.perform_search(term)
                links.extend(term_links)
                await self.page.wait_for_timeout(1000)
                
        except Exception as e:
            print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå: {e}")
        
        return links

    async def perform_search(self, search_term):
        """‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"""
        found_links = []
        
        try:
            # ‡∏´‡∏≤ search input
            search_selectors = [
                '#search', 'input[type="search"]', 'input[name="q"]', 
                'input[placeholder*="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"]', '.search-box input'
            ]
            
            search_input = None
            for selector in search_selectors:
                search_input = await self.page.query_selector(selector)
                if search_input:
                    break
            
            if search_input:
                await search_input.fill('')
                await search_input.fill(search_term)
                await search_input.press('Enter')
                await self.page.wait_for_timeout(3000)
                
                # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå
                links = await self.page.query_selector_all('a[href]')
                for link in links:
                    try:
                        href = await link.get_attribute('href')
                        text = await link.inner_text()
                        
                        if href and self.is_relevant_link(text, href):
                            full_url = urljoin(self.base_url, href)
                            found_links.append(full_url)
                    except:
                        continue
                        
        except Exception as e:
            print(f"     ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        
        return found_links

    async def analyze_website_structure(self):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡πá‡∏ö"""
        links = []
        
        try:
            await self.page.goto(self.base_url, wait_until='networkidle')
            await self.page.wait_for_timeout(2000)
            
            # ‡∏´‡∏≤‡πÄ‡∏°‡∏ô‡∏π‡∏´‡∏•‡∏±‡∏Å
            menu_selectors = [
                'nav a', '.menu a', '.navbar a', 'header a',
                '.main-menu a', '.navigation a', '.nav-link'
            ]
            
            for selector in menu_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    for element in elements:
                        href = await element.get_attribute('href')
                        text = await element.inner_text()
                        
                        if href and self.is_relevant_link(text, href):
                            full_url = urljoin(self.base_url, href)
                            links.append(full_url)
                except:
                    continue
                    
            # ‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            all_links = await self.page.query_selector_all('a[href]')
            for link in all_links:
                try:
                    href = await link.get_attribute('href')
                    text = await link.inner_text()
                    
                    if href and self.is_relevant_link(text, href):
                        full_url = urljoin(self.base_url, href)
                        if full_url not in links:
                            links.append(full_url)
                except:
                    continue
                    
        except Exception as e:
            print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: {e}")
        
        return links

    async def try_url_patterns(self):
        """‡∏•‡∏≠‡∏á URL patterns"""
        links = []
        
        patterns = [
            f"{self.base_url}/programs",
            f"{self.base_url}/courses", 
            f"{self.base_url}/search?q=computer",
            f"{self.base_url}/search?q=engineering",
            f"{self.base_url}/university",
            f"{self.base_url}/faculty",
            f"{self.base_url}/program",
            f"{self.base_url}/course"
        ]
        
        for pattern in patterns:
            try:
                await self.page.goto(pattern, wait_until='networkidle', timeout=10000)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
                content = await self.page.content()
                if "404" not in content and "not found" not in content.lower():
                    page_links = await self.page.query_selector_all('a[href]')
                    for link in page_links:
                        try:
                            href = await link.get_attribute('href')
                            text = await link.inner_text()
                            
                            if href and self.is_relevant_link(text, href):
                                full_url = urljoin(self.base_url, href)
                                links.append(full_url)
                        except:
                            continue
                            
            except Exception as e:
                print(f"     ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö: {pattern}")
                continue
        
        return links

    async def beautifulsoup_method(self):
        """‡πÉ‡∏ä‡πâ BeautifulSoup ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        links = []
        
        try:
            response = self.session.get(self.base_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    if self.is_relevant_link(text, href):
                        full_url = urljoin(self.base_url, href)
                        links.append(full_url)
                        
        except Exception as e:
            print(f"   ‚ùå BeautifulSoup ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        
        return links

    def is_relevant_link(self, text, href):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        if not text or not href:
            return False
        
        text_lower = text.lower()
        href_lower = href.lower()
        combined = f"{text_lower} {href_lower}"
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        has_keyword = any(keyword.lower() in combined for keyword in self.target_keywords)
        
        # ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        excluded = ['login', 'register', 'contact', 'about', 'news', 'facebook', 'twitter']
        has_excluded = any(ex in combined for ex in excluded)
        
        return has_keyword and not has_excluded

    async def extract_comprehensive_data(self, links):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        print(f"\nüìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {len(links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå...")
        
        success_count = 0
        
        for i, link in enumerate(links, 1):
            print(f"\nüìÑ [{i}/{len(links)}] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:")
            print(f"    üîó {link}")
            
            try:
                # ‡∏•‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ Playwright ‡∏Å‡πà‡∏≠‡∏ô
                program_data = await self.extract_with_playwright(link)
                
                if not program_data:
                    # ‡∏•‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ requests + BeautifulSoup
                    program_data = await self.extract_with_requests(link)
                
                if program_data:
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠ URL ‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
                    program_data['url_title'] = self.generate_url_title(program_data, link)
                    
                    self.programs_data.append(program_data)
                    success_count += 1
                    
                    print(f"    ‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                    print(f"       üè´ {program_data['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢']}")
                    print(f"       üìö {program_data['‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£'][:40]}...")
                    print(f"       üí∞ {program_data['‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°']:,} ‡∏ö‡∏≤‡∏ó" if program_data['‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°'] else "       üí∞ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°")
                else:
                    print(f"    ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                
                await asyncio.sleep(0.5)  # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
                
            except Exception as e:
                print(f"    ‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)[:40]}...")
                continue
        
        print(f"\nüéØ ‡∏™‡∏£‡∏∏‡∏õ: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {success_count}/{len(links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")

    async def extract_with_playwright(self, url):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Playwright"""
        try:
            await self.page.goto(url, wait_until='networkidle', timeout=15000)
            await self.page.wait_for_timeout(2000)
            
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            return self.extract_program_info(soup, url)
            
        except:
            return None

    async def extract_with_requests(self, url):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ requests"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                return self.extract_program_info(soup, url)
        except:
            pass
        return None

    def extract_program_info(self, soup, url):
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°"""
        try:
            program_data = {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': self.extract_university(soup, url),
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': self.extract_program_name(soup),
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': self.extract_tuition_info(soup),  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°' ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
                'URL': url
            }

            if (program_data['‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£'] != '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏' and 
                len(program_data['‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']) > 10):
                return program_data

            return None

        except Exception as e:
            return None


    def extract_university(self, soup, url):
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏à‡∏≤‡∏Å alt ‡∏Ç‡∏≠‡∏á‡πÇ‡∏•‡πÇ‡∏Å‡πâ"""
        print(f"    üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢...")

        # 1. ‡∏´‡∏≤ logo ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô class h-brand ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á alt
        img = soup.select_one('span.h-brand img[alt]')
        if img:
            alt_text = img.get('alt', '').strip()
            if alt_text.startswith('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢') and len(alt_text) < 100:
                print(f"    ‚úÖ ‡∏û‡∏ö‡∏à‡∏≤‡∏Å <img alt>: {alt_text}")
                return alt_text

        # 2. ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ fallback ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å
        print("    ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å <img alt>")
        return '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'


    def extract_program_name(self, soup):
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£"""
        # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        program_patterns = ['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', 'program', 'course']
        for pattern in program_patterns:
            elements = soup.find_all(text=re.compile(pattern, re.IGNORECASE))
            for element in elements:
                parent = element.parent
                if parent:
                    next_cell = parent.find_next_sibling()
                    if next_cell:
                        text = next_cell.get_text(strip=True)
                        if len(text) > 10 and any(kw.lower() in text.lower() for kw in self.target_keywords):
                            return text
        
        # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å heading
        headings = soup.find_all(['h1', 'h2', 'h3'])
        for heading in headings:
            text = heading.get_text(strip=True)
            if (len(text) > 10 and 
                any(kw.lower() in text.lower() for kw in self.target_keywords) and
                'TCAS' not in text):
                return text
        
        # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å title
        title = soup.find('title')
        if title:
            text = title.get_text(strip=True)
            if any(kw.lower() in text.lower() for kw in self.target_keywords):
                return text
        
        return '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'

    def extract_tuition_info(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏à‡∏≤‡∏Å <dt>‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢</dt> ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏≠‡∏≤ <dd> ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
        print("    üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢...")
        
        # ‡∏´‡∏≤ <dt> ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢"
        dt_elements = soup.find_all('dt')
        
        for dt in dt_elements:
            dt_text = dt.get_text(strip=True)
            if '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢' in dt_text:
                # ‡∏´‡∏≤ <dd> ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                dd = dt.find_next_sibling('dd')
                if dd:
                    tuition_text = dd.get_text(strip=True)
                    print(f"    ‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢: {tuition_text}")
                    return tuition_text
        
        print("    ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢")
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

    def generate_url_title(self, program_data, url):
        """‡πÉ‡∏ä‡πâ URL ‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"""
        return url

    async def use_sample_data(self):
        """‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á"""
        print("üìù ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô...")
        
        sample_data = [
            {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': '‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢',
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': '‡∏ß‡∏®.‡∏ö. ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•',
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': 35000,
                'URL': 'https://www.mytcas.com/program/chula-computer',
                'url_title': '‡∏à‡∏∏‡∏¨‡∏≤ - ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•'
            },
            {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå',
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': '‡∏ß‡∏®.‡∏ö. ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå',
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': 28000,
                'URL': 'https://www.mytcas.com/program/ku-computer',
                'url_title': '‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå - ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'
            },
            {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ',
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': '‡∏ß‡∏®.‡∏ö. ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå',
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': 32000,
                'URL': 'https://www.mytcas.com/program/kmutt-ai',
                'url_title': '‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ - ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå'
            },
            {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå',
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': '‡∏ß‡∏®.‡∏ö. ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå',
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': 30000,
                'URL': 'https://www.mytcas.com/program/tu-computer',
                'url_title': '‡∏ò‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå - ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'
            },
            {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏°‡∏´‡∏¥‡∏î‡∏•',
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': '‡∏ß‡∏ó.‡∏ö. ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå',
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': 45000,
                'URL': 'https://www.mytcas.com/program/mahidol-cs',
                'url_title': '‡∏°‡∏´‡∏¥‡∏î‡∏• - ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'
            }
        ]
        
        self.programs_data.extend(sample_data)
        print(f"‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á {len(sample_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    def save_to_excel_with_smart_urls(self, filename='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå_TCAS_‡∏ß‡∏¥‡∏®‡∏ß‡∏Ñ‡∏≠‡∏°.xlsx'):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡∏û‡∏£‡πâ‡∏≠‡∏° URL ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"""
        if not self.programs_data:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            return
        
        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(self.programs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£...")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_data = []
        for program in self.programs_data:
            row = {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': program.get('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': program.get('‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': program.get('‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°', None),
                'URL': program.get('URL', '')  # ‡πÉ‡∏ä‡πâ URL ‡∏à‡∏£‡∏¥‡∏á
            }
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        df = df.sort_values(['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°'], na_position='last')
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å
            df.to_excel(writer, sheet_name='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', index=False)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° hyperlink
            workbook = writer.book
            worksheet = writer.sheets['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á hyperlink ‡πÉ‡∏ô column D (URL)
            for row_idx, url in enumerate(df['URL'], start=2):
                if url and url.startswith('http'):
                    cell = worksheet.cell(row=row_idx, column=4)  # Column D
                    cell.hyperlink = url
                    cell.value = url  # ‡πÅ‡∏™‡∏î‡∏á URL ‡∏à‡∏£‡∏¥‡∏á
                    cell.style = "Hyperlink"
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            stats = self.create_stats(df)
            stats_df = pd.DataFrame(stats)
            stats_df.to_excel(writer, sheet_name='‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏£‡∏∏‡∏õ', index=False)
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            worksheet.column_dimensions['A'].width = 40  # ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢
            worksheet.column_dimensions['B'].width = 60  # ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
            worksheet.column_dimensions['C'].width = 15  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°
            worksheet.column_dimensions['D'].width = 60  # URL
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            stats_sheet = writer.sheets['‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏£‡∏∏‡∏õ']
            stats_sheet.column_dimensions['A'].width = 30
            stats_sheet.column_dimensions['B'].width = 60
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå {filename}")
        print("üîó ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'URL' ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á URL ‡∏à‡∏£‡∏¥‡∏á!")
        
        self.print_summary(df)

    def create_stats(self, df):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
        stats = []
        
        stats.append({'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î', '‡∏Ñ‡πà‡∏≤': len(df)})
        stats.append({'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏Ñ‡πà‡∏≤': df['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'].nunique()})
        
        fee_data = df['‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°'].dropna()
        if len(fee_data) > 0:
            stats.extend([
                {'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': '‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°', '‡∏Ñ‡πà‡∏≤': len(fee_data)},
                {'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏ö‡∏≤‡∏ó)', '‡∏Ñ‡πà‡∏≤': f"{fee_data.mean():,.0f}"},
                {'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (‡∏ö‡∏≤‡∏ó)', '‡∏Ñ‡πà‡∏≤': f"{fee_data.min():,.0f}"},
                {'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏ö‡∏≤‡∏ó)', '‡∏Ñ‡πà‡∏≤': f"{fee_data.max():,.0f}"},
            ])
        
        # Top ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢
        top_unis = df[df['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'] != '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏']['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'].value_counts().head(5)
        for i, (uni, count) in enumerate(top_unis.items(), 1):
            stats.append({'‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': f'‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö {i}', '‡∏Ñ‡πà‡∏≤': f"{uni} ({count} ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£)"})
        
        return stats

    def print_summary(self, df):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•"""
        print(f"\nüìà ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        print("=" * 70)
        
        total = len(df)
        with_fee = df['‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°'].notna().sum()
        universities = df[df['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'] != '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏']['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'].nunique()
        
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total}")
        print(f"üè´ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢: {universities}")
        print(f"üí∞ ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°: {with_fee}/{total}")
        
        if with_fee > 0:
            fee_data = df['‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°'].dropna()
            print(f"üíµ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {fee_data.mean():,.0f} ‡∏ö‡∏≤‡∏ó")
            print(f"üí≥ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {fee_data.min():,.0f} ‡∏ö‡∏≤‡∏ó")
            print(f"üíé ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {fee_data.max():,.0f} ‡∏ö‡∏≤‡∏ó")
        
        print(f"\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        display_df = df[['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°']].head(5)
        print(display_df.to_string(index=False))

async def main():
    scraper = UltimateTCASScraper()
    
    try:
        print("üöÄ ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TCAS Ultimate Version")
        print("=" * 70)
        print("üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö 4 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏û‡∏£‡πâ‡∏≠‡∏° URL ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°")
        print("üîß ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£: ‡πÉ‡∏ä‡πâ 4 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô + ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô browser
        await scraper.init_browser(headless=False, slow_mo=500)
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        await scraper.multi_method_data_collection()
        
        print(f"\n‚úÖ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à: {len(scraper.programs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if scraper.programs_data:
            scraper.save_to_excel_with_smart_urls()
            
            # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON
            with open('ultimate_tcas_data.json', 'w', encoding='utf-8') as f:
                json.dump(scraper.programs_data, f, ensure_ascii=False, indent=2)
            print("‚úÖ ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô ultimate_tcas_data.json")
            
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        
        print("\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
        print("üìä ‡πÑ‡∏ü‡∏•‡πå Excel ‡∏û‡∏£‡πâ‡∏≠‡∏° URL ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß!")
        print("‚è≥ ‡∏£‡∏≠ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
        await asyncio.sleep(10)
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.close_browser()

if __name__ == "__main__":
    asyncio.run(main())