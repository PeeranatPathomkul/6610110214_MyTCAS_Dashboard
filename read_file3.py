import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin, urlparse
import time
import json
from datetime import datetime
import requests

class TCASScraper:
    def __init__(self):
        self.base_url = "https://www.mytcas.com"
        self.programs_data = []
        self.browser = None
        self.page = None
        self.session = requests.Session()
        
        # User agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
        self.target_keywords = [
            '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', 'computer', '‡∏Ñ‡∏≠‡∏°',
            '‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå', 'artificial intelligence', 'ai',
            '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°', 'engineering'
        ]

    async def init_browser(self, headless=False, slow_mo=500):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô browser"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô browser...")
        
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=headless,
            slow_mo=slow_mo,
            args=['--start-maximized', '--no-sandbox', '--disable-blink-features=AutomationControlled']
        )
        
        self.context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        
        self.page = await self.context.new_page()
        self.page.set_default_timeout(30000)
        
        print("‚úÖ Browser ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")

    async def close_browser(self):
        """‡∏õ‡∏¥‡∏î browser"""
        try:
            if self.browser:
                await self.browser.close()
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: {e}")

    async def search_via_website(self):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå"""
        print("üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå...")
        links = []
        
        try:
            await self.page.goto(self.base_url, wait_until='networkidle')
            await self.page.wait_for_timeout(3000)
            
            search_terms = [
                '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå',
                'computer engineering', 
                '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå',
                'artificial intelligence engineering',
                'ai engineering',
                'com engineering'
            ]
            
            for term in search_terms:
                print(f"   üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {term}")
                term_links = await self.perform_search(term)
                links.extend(term_links)
                await self.page.wait_for_timeout(1000)
                
        except Exception as e:
            print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå: {e}")
        
        # ‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ã‡πâ‡∏≥
        unique_links = list(set(links))
        print(f"üìä ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(unique_links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥)")
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå
        if unique_links:
            await self.extract_comprehensive_data(unique_links)
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå")

    async def perform_search(self, search_term):
        """‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"""
        found_links = []
        
        try:
            # ‡∏´‡∏≤ search input
            search_selectors = [
                '#search', 'input[type="search"]', 'input[name="q"]', 
                'input[placeholder*="‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"]', '.search-box input'
            ]
            
            search_input = None
            for selector in search_selectors:
                search_input = await self.page.query_selector(selector)
                if search_input:
                    break
            
            if search_input:
                await search_input.fill('')
                await search_input.fill(search_term)
                await search_input.press('Enter')
                await self.page.wait_for_timeout(3000)
                
                # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå
                links = await self.page.query_selector_all('a[href]')
                for link in links:
                    try:
                        href = await link.get_attribute('href')
                        text = await link.inner_text()
                        
                        if href and self.is_relevant_link(text, href):
                            full_url = urljoin(self.base_url, href)
                            found_links.append(full_url)
                    except:
                        continue
                        
        except Exception as e:
            print(f"     ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        
        return found_links

    def is_relevant_link(self, text, href):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        if not text or not href:
            return False
        
        text_lower = text.lower()
        href_lower = href.lower()
        combined = f"{text_lower} {href_lower}"
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        has_keyword = any(keyword.lower() in combined for keyword in self.target_keywords)
        
        # ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        excluded = ['login', 'register', 'contact', 'about', 'news', 'facebook', 'twitter']
        has_excluded = any(ex in combined for ex in excluded)
        
        return has_keyword and not has_excluded

    async def extract_comprehensive_data(self, links):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        print(f"\nüìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {len(links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå...")
        
        success_count = 0
        
        for i, link in enumerate(links, 1):
            print(f"\nüìÑ [{i}/{len(links)}] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:")
            print(f"    üîó {link}")
            
            try:
                # ‡∏•‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ Playwright ‡∏Å‡πà‡∏≠‡∏ô
                program_data = await self.extract_with_playwright(link)
                
                if not program_data:
                    # ‡∏•‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ requests + BeautifulSoup
                    program_data = await self.extract_with_requests(link)
                
                if program_data:
                    self.programs_data.append(program_data)
                    success_count += 1
                    
                    print(f"    ‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                    print(f"       üè´ {program_data['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢']}")
                    print(f"       üìö {program_data['‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£'][:50]}...")
                    print(f"       üí∞ {program_data['‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°']}")
                else:
                    print(f"    ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                
                await asyncio.sleep(0.5)  # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
                
            except Exception as e:
                print(f"    ‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)[:50]}...")
                continue
        
        print(f"\nüéØ ‡∏™‡∏£‡∏∏‡∏õ: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {success_count}/{len(links)} ‡∏•‡∏¥‡∏á‡∏Å‡πå")

    async def extract_with_playwright(self, url):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Playwright"""
        try:
            await self.page.goto(url, wait_until='networkidle', timeout=15000)
            await self.page.wait_for_timeout(2000)
            
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            return self.extract_program_info(soup, url)
            
        except:
            return None

    async def extract_with_requests(self, url):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ requests"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                return self.extract_program_info(soup, url)
        except:
            pass
        return None

    def extract_program_info(self, soup, url):
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°"""
        try:
            program_data = {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': self.extract_university(soup, url),
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': self.extract_program_name(soup),
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': self.extract_tuition_info(soup),
                'URL': url
            }

            if (program_data['‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£'] != '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏' and 
                len(program_data['‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']) > 10):
                return program_data

            return None

        except Exception as e:
            return None

    def extract_university(self, soup, url):
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏à‡∏≤‡∏Å alt ‡∏Ç‡∏≠‡∏á‡πÇ‡∏•‡πÇ‡∏Å‡πâ"""
        print(f"    üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢...")

        # ‡∏´‡∏≤ logo ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô class h-brand ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á alt
        img = soup.select_one('span.h-brand img[alt]')
        if img:
            alt_text = img.get('alt', '').strip()
            if alt_text.startswith('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢') and len(alt_text) < 100:
                print(f"    ‚úÖ ‡∏û‡∏ö‡∏à‡∏≤‡∏Å <img alt>: {alt_text}")
                return alt_text

        print("    ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å <img alt>")
        return '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'

    def extract_program_name(self, soup):
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£"""
        # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        program_patterns = ['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', 'program', 'course']
        for pattern in program_patterns:
            elements = soup.find_all(text=re.compile(pattern, re.IGNORECASE))
            for element in elements:
                parent = element.parent
                if parent:
                    next_cell = parent.find_next_sibling()
                    if next_cell:
                        text = next_cell.get_text(strip=True)
                        if len(text) > 10 and any(kw.lower() in text.lower() for kw in self.target_keywords):
                            return text
        
        # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å heading
        headings = soup.find_all(['h1', 'h2', 'h3'])
        for heading in headings:
            text = heading.get_text(strip=True)
            if (len(text) > 10 and 
                any(kw.lower() in text.lower() for kw in self.target_keywords) and
                'TCAS' not in text):
                return text
        
        # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å title
        title = soup.find('title')
        if title:
            text = title.get_text(strip=True)
            if any(kw.lower() in text.lower() for kw in self.target_keywords):
                return text
        
        return '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'

    def extract_tuition_info(self, soup):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏à‡∏≤‡∏Å <dt>‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢</dt> ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏≠‡∏≤ <dd> ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
        print("    üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢...")
        
        # ‡∏´‡∏≤ <dt> ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢"
        dt_elements = soup.find_all('dt')
        
        for dt in dt_elements:
            dt_text = dt.get_text(strip=True)
            if '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢' in dt_text:
                # ‡∏´‡∏≤ <dd> ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                dd = dt.find_next_sibling('dd')
                if dd:
                    tuition_text = dd.get_text(strip=True)
                    print(f"    ‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢: {tuition_text}")
                    return tuition_text
        
        print("    ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢")
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

    def save_to_excel(self, filename='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•_TCAS_‡∏ß‡∏¥‡∏®‡∏ß‡∏Ñ‡∏≠‡∏°.xlsx'):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel"""
        if not self.programs_data:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            return
        
        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(self.programs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£...")
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_data = []
        for program in self.programs_data:
            row = {
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': program.get('‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
                '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': program.get('‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
                '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°': program.get('‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'),
                'URL': program.get('URL', '')
            }
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        df = df.sort_values(['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'], na_position='last')
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Excel
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', index=False)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° hyperlink
            workbook = writer.book
            worksheet = writer.sheets['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á hyperlink ‡πÉ‡∏ô column D (URL)
            for row_idx, url in enumerate(df['URL'], start=2):
                if url and url.startswith('http'):
                    cell = worksheet.cell(row=row_idx, column=4)  # Column D
                    cell.hyperlink = url
                    cell.value = url
                    cell.style = "Hyperlink"
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            worksheet.column_dimensions['A'].width = 40  # ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢
            worksheet.column_dimensions['B'].width = 60  # ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
            worksheet.column_dimensions['C'].width = 20  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°
            worksheet.column_dimensions['D'].width = 60  # URL
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå {filename}")
        print("üîó ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'URL' ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢!")
        
        self.print_summary(df)

    def print_summary(self, df):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•"""
        print(f"\nüìà ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        print("=" * 50)
        
        total = len(df)
        universities = df[df['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'] != '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏']['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'].nunique()
        
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total}")
        print(f"üè´ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢: {universities}")
        
        print(f"\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        display_df = df[['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°']].head(5)
        print(display_df.to_string(index=False))

async def main():
    scraper = TCASScraper()
    
    try:
        print("üöÄ ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TCAS")
        print("=" * 50)
        print("üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞ AI")
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô browser
        await scraper.init_browser(headless=False, slow_mo=500)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        await scraper.search_via_website()
        
        print(f"\n‚úÖ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à: {len(scraper.programs_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if scraper.programs_data:
            scraper.save_to_excel()
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        
        print("\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
        print("‚è≥ ‡∏£‡∏≠ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
        await asyncio.sleep(10)
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.close_browser()

if __name__ == "__main__":
    asyncio.run(main())